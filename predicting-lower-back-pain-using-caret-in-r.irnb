{"cells":[{"metadata":{"_uuid":"aaf9a50501ffc2862c83b12873353aa17ce55e30"},"cell_type":"markdown","source":"# Predicting lower back pain using caret in R\nWe are going to have an explore of this data and then predict lower back pain using these biometric measurements; this is a **binary classification** problem, as we have two outcomes: *abnormal* and *normal*. \n\nThere are many ways that we can approach a classification problem, and there are many algorithms that can be used to create predictions. In this report, I use logistic regression, support vector machines and random forests. I compare the accuracies, and provide a method for looking at other predictive metrics and what they mean when evaluating a model.  \n\nI use the `caret` package to apply cross-validation to the data set to provide a better look at what the out-of-sample predictive power of the models are.\n\nIf you have any suggestions for improvements, or where I might be able to go next, then I would love to hear from you. Happy reading! \n\n- - - -"},{"metadata":{"_uuid":"dc0ce909ea52cec93b296f6d8c57471d85a7265f"},"cell_type":"markdown","source":"# Initial exploration and data loading\nFirst of all, let's start with loading the packages we are going ot us as well as the data while getting a feel for its structure. "},{"metadata":{"_uuid":"53c9c7834ee5187f1776e34229cbdeac69fa0fad","_execution_state":"idle","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"library(tidyverse)\nlibrary(dplyr)\nlibrary(readr)\n\ndata <- readr::read_csv('../input/Dataset_spine.csv')\nhead(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05d83825e2306c5b5a3cf61464f4dc888c408cf6"},"cell_type":"markdown","source":"We should note that there are no column names in the csv file itself, so we could add those manually using the information that has been provided with the dataset. \n\nWe can also see that we have 12 numeric columns, and 2 character columns; the 12 numeric/double columns represent those features that we are going to use to make our predictions, while the 2 character columns represent our classes, and further notes about the data set. \n\nNow that we have read in our data, we should begin with the usual exploratory analysis and visualisations to get a *feel* for the data, and an idea about which featuers may act as good indicators of lower back pain. "},{"metadata":{"trusted":true,"_uuid":"35fb773680f5464d9c1bff5acca1fa97e0c8c378"},"cell_type":"code","source":"# Check the dimensions of the data\ndim(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf6a35814dc455d549cdb451dfc8de787bc7d59d"},"cell_type":"markdown","source":"We only have 310 observations, which is a very small sample; it does mean that we aren't going to suffer too heavily with computational issues when training our models. However, with such a small sample size, we probably can't be certain about how well our results would extend to new cases. "},{"metadata":{"trusted":true,"_uuid":"8ebcf2891797285dc51fa75a70dbb3b6beb5a76c"},"cell_type":"code","source":"# Type cast the classification column\ndata$Class_att <- factor(data$Class_att)\n\n# Summarise our data \nsummary(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a036c1d27787ffb931e5fa73d5da012fde23f1a7"},"cell_type":"markdown","source":"A quick glimpse over the data shows us that we have 12 numeric columns across varying scales, and magnitudes. If we are going to use a KNN approach it is definitely going to be necessary to normalise the features so that no one feature has more weight over the classfication. \n\nWe can also see that we have a split of 210/100 for abnormal/normal. This shouldn't be an issue, and is certainly a welcome change from the small count rare events that are often associated with binary classification problems. \n\nWe can also see that we don't have any `NA` values in oru numeric columns, so we won't be required to do any imputation or cleaning where that is concerned. This can be confirmed with a little `NA` check. "},{"metadata":{"trusted":true,"_uuid":"9d96d418cb61a93c4f4c2051a77de32f0d4844c7"},"cell_type":"code","source":"sapply(data, FUN = function(col) sum(is.na(col)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d566494e90df5e86ed5e61d706b44b8a231b127e","_kg_hide-output":true},"cell_type":"code","source":"# Load GGally for a correlation plot\nlibrary(GGally)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5e282de3832774440865d54435586e4ed12a166"},"cell_type":"code","source":"# Remove non-numeric columns before observing correlations between variables\ndata %>% select_if(is.numeric) %>% ggcorr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85b2f2807291d2ec0db91d30c6eeb2ad728afcdd"},"cell_type":"markdown","source":"I think this is a good starting point to get an idea of how these variable are connected. We can see that there is definitely some correlation between the variables `Col1` - `Col4` which may be more obvious why when I add the column names from the meta data. This highlights the possibility for colinearity later in the analysis that we should be aware of, and perhaps for the fact that we might not need to use all of those variables as predictors. \n\nThe other columns don't seem to be correlated with one another, or at least not strongly. So what matters next is how they are related to the response variable. \n\nIf we look closer at our 14th column, we will see that this is actually being used to hold information about the other columns and what they represent. "},{"metadata":{"trusted":true,"_uuid":"f96af06d718811d52903f5d106803d8ac16e3074"},"cell_type":"code","source":"unique(data$X14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf817495b8451cabdfc9dc25db654344fa58f838"},"cell_type":"code","source":"# Assign the new column names using data from `X14`\ndata <- data %>% \n    dplyr::rename('pelvic_incidence' = Col1, \n           'pelvic_tilt' = Col2, \n           'lumbar_lordosis_angle' = Col3,\n           'sacral_slope' = Col4, \n           'pelvic_radius' = Col5, \n           'degree_spondylolisthesis' = Col6, \n           'pelvic_slope' = Col7, \n           'direct_tilt' = Col8, \n           'thoracic_slope' = Col9, \n           'cervical_tilt' = Col10, \n           'sacrum_angle' = Col11, \n           'scoliosis_slope' = Col12) %>%\n    dplyr::select(-X14)\n\nhead(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af736b67c7fbc640db59dde7d5556b9481f7158e"},"cell_type":"code","source":"# Reshape the data for plotting\ndata_long <- data %>%\n    gather('feature', 'value', -Class_att)\n\nhead(data_long)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"113ea6560deb3ba6767fddc36fe71b22ef09f669"},"cell_type":"markdown","source":"Transforming the data to long format can sometimes make the plotting syntax a little easier. This allows for a simpler plot creation and means that we can make use of the facetting capabilites of `ggplot2`; `facet_wrap` in this example. "},{"metadata":{"trusted":true,"_uuid":"6cd714598f731ade6b39f7ea2058e9fbffd5cfa6"},"cell_type":"code","source":"data_long %>%\n    ggplot(aes(Class_att, value, colour = Class_att)) + \n        geom_boxplot(alpha = 0.5) + \n        facet_wrap(~ feature, scales='free_y', ncol = 3) + \n        labs(x = NULL, y = NULL) + \n        theme_minimal()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92c90dd9e448fe13516edf2809f94f635c5f8855"},"cell_type":"markdown","source":"I find this is a good way to visualise the differences of each feature and our outcome class. Another alternative is to look at the difference in the distributions by using `geom_density`, which I have done below.  \n\n**Note**: In the above code I have used `free_y` on the scales as our variables aren't all using the same magnitude of values, so freeing the y axis range allows us to compare the individual variables much more easily. \n\n"},{"metadata":{"trusted":true,"_uuid":"eb93ce18073cbb4b9d254bbf480a7a3b730854c0"},"cell_type":"code","source":"data_long %>%\n    ggplot(aes(value, fill = Class_att)) + \n        geom_density(alpha = 0.5) + \n        facet_wrap(~ feature, scales='free', ncol = 3) + \n        labs(x = NULL, y = NULL) + \n        theme_minimal()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17e7e6b51cb3fa47266d7c2ac2d86c68015996ab"},"cell_type":"markdown","source":"Looking at the information using the `geom_density` provides more information about the differences between these values and the outcome variable. We are looking for two more distinct populations between the *normal* observations and the *abnormal* as these would be the features that grant us the most predictive power. \n\nIf those that differ the most between *normal* and *abnormal* are also highly correlated with one another then we may have found a good set of indicators that lead to lower back pain; the colinearity between the values most associated with the lower back pain could represent a single condition/issue. \n\nWe can also look at this by doing some calculations, and see by how much the values differ between the two classes of `Class_att`.  Doing this we can put a numeric value on the difference between the average values for each of our variables. "},{"metadata":{"trusted":true,"_uuid":"4bfbb248b7c0211261f978a5d5b287d3cfde902f"},"cell_type":"code","source":"data_long %>%\n    dplyr::group_by(feature, Class_att) %>%\n    dplyr::summarise(mean = mean(value)) %>%\n    spread(Class_att, mean) %>% # Spread the data to make the table 'wide'\n    dplyr::mutate(Diff = Abnormal - Normal, \n           `Percentage Difference` = (Abnormal - Normal) / Normal * 100) %>%\n    arrange(desc(abs(`Percentage Difference`))) # Arrange by the largest differences, accounting for negatives","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a588073ff9fd955c9704eec5652ad6dbdb8679e9"},"cell_type":"markdown","source":"We observe the differences between the *Abormal* and *Normal* mean average observations. As the units and magnitudes for the measurements are differing between the variables, we calculate the difference via a percentage to make them more easily comparable. We can see that `degree_spondylolisthesis` is, on average, much larger in sufferers than it is in non-sufferers. `pelvic_tilt` is the next largest difference. "},{"metadata":{"_uuid":"c25c582e9d7b7756fe02da9ed897bd60ee75a3a5"},"cell_type":"markdown","source":"# Making predictions\nTo make it easier to intepret the results of the predictions, I am going to mutate a new variable that we will use for our predictions. `is_abnormal` will represent whether the class of that particular individual/observation was *Abnormal*.\n\n**Note**: *further down this kernel I revert using this variable and go back to using the `Class_att` variable. This is because I ran into issues with `caret` where the factor levels essentially can't be 0 and 1 and must be proper names. *"},{"metadata":{"trusted":true,"_uuid":"2cbec6d725c8bf4ff0dccf2bc626fed228bea553"},"cell_type":"code","source":"data <- data %>% dplyr::mutate(is_abnormal = factor(ifelse(Class_att == 'Abnormal', 1, 0)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ae8506ce25e4b3958c785f2a3e27cfd5d0b1a66"},"cell_type":"markdown","source":"I'll remove the old `Class_att` column from the data for the next model building step. We will be focussing on the new mutated `is_abnormal` and using that as our response variable. I am retaining the original data with both numerically coded and string coded classes for future use. This is good practice as it means that you don't have to reload your data should you make a mistake and overwrite it. "},{"metadata":{"trusted":true,"_uuid":"61f3f25a8519241092709708b09c97698b42bd39"},"cell_type":"code","source":"log_test_data <- data %>% dplyr::select(-Class_att)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3aece891ce89ff551254b93d55e1309bd46c726"},"cell_type":"code","source":"# View the head of our new data\nhead(log_test_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf47bb3ca9be37de5a185b29a9e18794f2147ff8"},"cell_type":"markdown","source":"## Logistic Regression with `glm`\nLogisitic regression is often the starting point for binary classification problems, and it will be mine here. It is a linear model that uses a logit function to produce an output that is between 1 and 0; this acts as our prediction on which of the classes the data is likely to be from.  \n\nWe use `glm` here, with the `family = binomial` which indicates that we are approaching a binary outcome prediction, and thus use logistic regression. "},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"593edd8d88a1d4f23fae156f9d312dc930b0a524"},"cell_type":"code","source":"logistic_regression_model <- glm(is_abnormal ~ ., data = log_test_data, family = binomial)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd96567479d7c9b7ce3447a5b23daa88099032ed"},"cell_type":"markdown","source":"Now that we have created our model using logisitic regression, let's have a look at the summary information that we can see. "},{"metadata":{"trusted":true,"_uuid":"30aa578156e103a5aa9a118242872a84d218d8b9"},"cell_type":"code","source":"summary(logistic_regression_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fde8a3bfd92a224351bd8fb23b4d4ae5b13bf52"},"cell_type":"markdown","source":"`pelvic_radius` and `degree_spondylolisthesis` are the significant features here. This makes sense given the difference that we saw for `degree_spondylolisthesis`, and looking back at our boxplot graphs there appears to be a noticeable differnence in the `pelvic_radius` which the model has picked out. Let's do a quick check and see what the accuracy of this model is. "},{"metadata":{"_uuid":"4f7d31601bc68568d4d381616143bb2bf331c657"},"cell_type":"markdown","source":"# Evaluating our models and predictions\nWe can use the `augment` function from the `broom` package (linked [here](https://cran.r-project.org/web/packages/broom/vignettes/broom.html)) to add predictions to our data, which we will later use to compare the results from our model to that of the known observational classes.  "},{"metadata":{"trusted":true,"_uuid":"32264b76e4583d7b5221695a88b237396d15f06f"},"cell_type":"code","source":"library(broom)\naugment(logistic_regression_model, newdata = data, type.predict = \"response\") %>% \n    head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff925f2498f185433284a9c22ce7eddf6a7038b6"},"cell_type":"markdown","source":"I will go one step further to have a look at the confusion matrix, and get a feel of the performance of our model by comparing the prediction results against the actual values for the classes: *Abnormal* or *Normal*."},{"metadata":{"trusted":true,"_uuid":"bfd80e9cd371a0891da3462c973f38446fca024c"},"cell_type":"code","source":"augment(logistic_regression_model, type.predict = \"response\") %>% \n    mutate(abnormal_pred = round(.fitted)) %>%\n    select(is_abnormal, abnormal_pred) %>%\n    table()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd153244a3c07a4451f9aa709caae81f938ae20d"},"cell_type":"markdown","source":"A really useful package for evaluation models that I've found is `Metrics` (linked [here](https://cran.r-project.org/web/packages/Metrics/Metrics.pdf)) that can provide us an output of the accuracy by comparing the predicted vs the actual.  \n\nWe could work this out for ourselves by summing the diagonals against the total number of observations, but this allows for much more readable code using the function `accuracy`. "},{"metadata":{"trusted":true,"_uuid":"ace7f83b9da8c33ecf7e9730ed56265afebd903f"},"cell_type":"code","source":"library(Metrics)\npredictions <- augment(logistic_regression_model, type.predict = \"response\") %>% \n                mutate(abnormal_pred = round(.fitted)) %>% \n                select(is_abnormal, abnormal_pred)\n\n# Compare the predictions vs. observed\naccuracy(predictions$is_abnormal, predictions$abnormal_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5c7373f78fa7d776a9ca18ebab92c93461e8473"},"cell_type":"markdown","source":"One step further than `Metrics::accuracy` is using `caret::confusionMatrix` which returns, not only, a table like the one we created earlier but a wider range of metrics and measures that give deeper insight into a model's performance. I go through the main ones below to give you a flavour for them, as well as provide a link to find out more. "},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"c91969dd8276f3c7c82dc827ce6a5b37b7bbcf92"},"cell_type":"code","source":"library(lattice) # lattice is required \nlibrary(caret)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8138b8db511576c3b07b28a3d413a1076c81bb4"},"cell_type":"code","source":"caret::confusionMatrix(factor(predictions$abnormal_pred), \n                       predictions$is_abnormal, \n                       positive = \"1\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"188bd69effc5ae43451fe2f2a9b239ee0f5e824b"},"cell_type":"markdown","source":"Note that I have indicated that the positive result is \"1\" as that indiciates the presence of back pain in our set up. If you were using named classes then you would pass in the level that indicates the positive class i.e. `positive = \"Abnormal\"`\n\n***Sensitivity (sometimes called recall/PPV)*** is the true positive rate, i.e. the rate at which you can predict the positive results. It is the proportion of positive results that were identified as such.  \n***Specificity/NPV*** is the false positive rate, i.e. the rate at which you can predict the negative results (*Normal* classes in this case). Measures the proportion of actual negative observations that were identified as such. \n***Prevalence***  is the proportion of observations that were the 'positive' result. In our case this was 210 *Abnormal* / 310 *Total*. \n\nA detailed breakdown of how these values are calculated can be found [here](https://www.rdocumentation.org/packages/caret/versions/6.0-80/topics/confusionMatrix). \n\nIt will depend on the model, the situation and the data as to which of these measures is the most important. If you cannot afford to miss a positive case, then something like the sensitivity is going to be very important as you will want to correctly predict all the observed positive cases. Precision would also be worth investigating in that instance. "},{"metadata":{"_uuid":"9655158bbf9911f6d71632c3be15b6687b1f2554"},"cell_type":"markdown","source":"# Cross validation for out-of-sample accuracies\nOne thing that I'm sure many of you will have noticed is that I didn't enforce a test/train split of the data when building the above model. Splitting your available data is good practice as it gives you an insight as to how the model will perform when it comes up against unseen data.  \n\nGenerally speaking, testing your model on the data that has been used to train it inflates the accuracy and gives you an unrealistic image of model performance as it is possible that you have overfit the data; this tends to be particularly true of random forest models. \n\nWe can tackle this through creating a test/train split or using cross validation. I will opt for the latter and will using the `caret` package to do so. \n\n**Note**: *we want to include the class probabilities in our `trainControl` so that we can use them later to generate ROC curves and AUC. For this to be allowed, our class can't have the values \"0\" and \"1\". It throws an error, so I will revert back to the original data frame and use the `Class_att` as our response variable. *\n\n**Note**: *I make it seem that splitting the data and using cross validation aren't allowed to happen at the same time. I actually think, having made this kernel, that splitting the data and then performing cross validation on a training set while building a model and then having a test set for a final test is the best process. When you perform cross validation with caret it then builds a final model using the full data set that you've passed in (this got me into confusion and trouble later on).*"},{"metadata":{"trusted":true,"_uuid":"47b1e348630c1e1f9a64c59a52a15bceccf77b53"},"cell_type":"code","source":"caret_data <- data %>% select(-is_abnormal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7acc7e3c79dde3d8924cfca0b6a97c286a7b572"},"cell_type":"code","source":"# Set the random seed for reproducability\nset.seed(1)\n\n# Create a train control to indiciate the cross validation amount\ntrain_control <- caret::trainControl(method = 'cv', number = 5, savePredictions = 'final', classProbs = TRUE) \n\n# Build the model - here we using the glm method which is what we previously used\nlog_model_caret <- caret::train(Class_att ~ ., \n                         data = caret_data, \n                         trControl = train_control, \n                         method = 'glm', \n                         family = 'binomial')\n\nprint(log_model_caret)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d05565c99b23d22d2419995dd754fa7a5accfe00"},"cell_type":"markdown","source":"Our accuracy is 83.54%, which is lower than we had previously observed, but not drastically different. We can see that resampling occurred though; 5 times (as specified by our `trainControl` above.\n\nWe can use the same `train_control` with other models now and compare how other predictions models perform in comparison to the humble logistic regression for our needs. I have access to the predictions for this model as I have specified the `savePredictions` argument when defining `train_control`.  \n\nUsing `final` saves the predictions for the optimal tuning parameters; this doesn't mean a great deal here but is important for other algorithms that use a tuning grid to control the hyperparameters when building the model. "},{"metadata":{"trusted":true,"_uuid":"6e567cc8c032ea2547123bedf45be55a60d635cf"},"cell_type":"code","source":"caret::confusionMatrix(log_model_caret$pred$pred, log_model_caret$pred$obs, positive = \"Abnormal\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fa178da381d8ce4e2bfab9f6447ca0584ed389f"},"cell_type":"markdown","source":"## Support vector machine model\nNow that we have created our logistic regression model, we can use another library through `caret`, namely the `svmLinear` method to create a support vector machine to classify our data. "},{"metadata":{"trusted":true,"_uuid":"d50dbad17ce1ca3911f9a148d05f8f1b93c912af"},"cell_type":"code","source":"svm_model_caret <- caret::train(Class_att ~ ., \n                         data = caret_data, \n                         trControl = train_control, # Note use of same trControl\n                         method = 'svmLinear', \n                         tuneLength = 5)\nprint(svm_model_caret)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"835fdd2e05b44f5057ce6c668562b0733c718a6d"},"cell_type":"markdown","source":"The initial accuracy looks to be 83.87% which is a very slight improvement over our first logistic regression model. Let's have a deeper look at the results by using the `caret::confusionMatrix` function as we did previously. "},{"metadata":{"trusted":true,"_uuid":"0e81cd8c5c0415768861f6cb28452ac1d984b563"},"cell_type":"code","source":"caret::confusionMatrix(svm_model_caret$pred$pred, svm_model_caret$pred$obs, positive = \"Abnormal\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9551b89a873ff9ae19c81c6fdc3f82055ba3d923"},"cell_type":"markdown","source":"## Random forest model\nOur SVM model appeared to have a higher predictive power, having both a higher general accuracy as well as producing less false positives and higher balanced accuracy. I want to try different models, to try and find an optimal solution; with such a small sample size, it is unlikely that we are going to reach computational limitations. "},{"metadata":{"trusted":true,"_uuid":"8c956fdc715c634591a28291aacfbb05166f4ad5"},"cell_type":"code","source":"rf_model_caret <- caret::train(Class_att ~ ., \n                         data = caret_data, \n                         trControl = train_control, \n                         method = 'rf', \n                         tuneLength = 10) # Set to force higher number of tried mtry values\nprint(rf_model_caret)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c7b23a0ced3963b5dca02909eb0561f39279c87"},"cell_type":"code","source":"caret::confusionMatrix(rf_model_caret$pred$pred, rf_model_caret$pred$obs, positive = \"Abnormal\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80614d0d310210511326b3e614936378c282d44d"},"cell_type":"markdown","source":"Our overall accuracy hasn't improved, but we may want to focus on other metrics. Sensitivity could be important here for realistic applications; it means that we would have missed fewer individuals that suffer from back pain. It is hard to say that for certain as I don't have a full understanding of the data's applications to patient care, but I am just highlighting some differences between models that have similar overall accuracies. \n\n- - - -"},{"metadata":{"_uuid":"6eb3486ac54de1206274bd6b6c51ea8fbd52dc66"},"cell_type":"markdown","source":"# ROC curves and further model comparison\nThe ROC curve is a way of visualising a predictive model by applying a range of splitting prediction thresholds to give us our predicted classes. i.e. When we are making a prediction we are given a probability between 0 and 1 as to whether that observation belongs in a given class.  \n\nDepending where we apply the classification threshold will ultimately decide which class our model predicts. If we have a threshold of 0.5 (anything above 0.5 is given the class of 1) then 0.55 would be predicted to be class 1. However, if we changed our threshold to 0.75, then the same observation would instead be classified as the class 0. The ROC curve shows how altering this threshold impacts our model's performance.  \n\nLet's have a look at one. "},{"metadata":{"_uuid":"511ea9f024a4d20a6f1d10c125f144ac7c62ea89"},"cell_type":"markdown","source":"## Extracting the class probabilities for our predictions\nIn order to evaluate the threshold levels for the predictions, we have to know the probability value that the model gave for each observation. So far we have just been working with the class output itself. \n\nLet's have a look at the probability outputs for the models that we have created. \n\n**Note**: *here I am using the FULL FINAL MODEL to predict the classes of the full dataset on which the model was trained. It took me a while to realise why these measures provided a higher degree of performance than our model summary ones. This is why. The model summary results are averaged over our cross validations, where test data is used to measure performance on a model built from the remainder of the data. When that is done, a figure is given which represents an estimate of the out-of-sample performance, before a final model is made using the full data.\nThis would be why having another test/train split outside of cross validation would be advantageous. *"},{"metadata":{"trusted":true,"_uuid":"fd44769917390bb799d86b7ab6a3460e0f673de2"},"cell_type":"code","source":"models <- list(rf = rf_model_caret, svm = svm_model_caret, lr = log_model_caret)\npredictions <- predict(models, type = 'prob')\nlapply(predictions, head)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e366e1d4cfb5ec9865e482e8d1828f7c7fe32994"},"cell_type":"markdown","source":"## Random Forest ROC curve\nI was having issues with the ROC curve for the random forest, with the output being 1. I knew this couldn't be true as I hadn't had a perfect model accuracy when observing the model's summary post-building.  \n\nI haven't quite managed to work out why it was producing the result below, but I think it has something to do with the final model for the `caret::train()` including the full data set with the parameters found to be best during the cross-validation phase. Then, when I have come to plot using the data, it has overfitted and produced a perfect fit for that information, which differs from the values produced during the out-of-sample estimations while the cross-validation occurs. (**I have explained above why the results differ, I worked it out!**)\n\nI suppose the perfect method for model building and testing, would be to do a test/train split initially. Do cross-validation on the training data, and then build a final model using the entire set. Test on the testing data which has had no part in the training process at all. Food for thought. \n\n### Incorrect ROC curve"},{"metadata":{"trusted":true,"_uuid":"593ad4b8929795ccf1c23881d0c93c7a94618567"},"cell_type":"code","source":"library(caTools)\n# Note using the predictions list\ncolAUC(predictions$rf$Abnormal, caret_data$Class_att, plotROC = TRUE)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"40e40cbec06ae2d7eaffadee3d1fb3b1cee0d434"},"cell_type":"code","source":"library(pROC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e87b6db762b741c957cb57b4c0b22617d7fcc159"},"cell_type":"code","source":"# Note using predict function\nroc(predictor=predict(rf_model_caret, type='prob')$Abnormal,\n               response=caret_data$Class_att,\n               levels=rev(levels(caret_data$Class_att)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bc83030de57103df36dd6e36dd98fcf435263b7"},"cell_type":"markdown","source":"When I calculate the AUC separately from the plot, we are getting 1. It only occurs for this model, and that is probably because we have the tuning parameter of `mtry`. It should include a specific selection of `mtry` and if you don't select a specific version then it aggregates all of the versions and we get inaccuracies. (**Not strictly true, just coincidence**)\n\nLet's select just a specific set of predictions, those with `mtry = 8` to see if we can get a new prediction. "},{"metadata":{"_uuid":"6fac9809c878c2e822f809b004a024ebeffebfbe"},"cell_type":"markdown","source":"I can then take the predictions from the model where the mtry was 8. If I plot these using the `pROC` library, we can see a ROC curve that I would imagine to be much more accurate in reflecting the model's performance. \n\nSelecting the predictions for a particular mtry value, so that we have the correct number of predictions for our comparison. "},{"metadata":{"trusted":true,"_uuid":"b0e04bc0c53b0f9bce0e7d190e9eadd36c11dfa8"},"cell_type":"code","source":"# Select the mtry that was said to have the highest accuracy, 8 in our case\nindices <- rf_model_caret$pred$mtry == 8\n\n# Plot the roc curve for those particular values\n# More realistic ROC curve - note the use of the model's stored results created during training \n# rather than later predictions\nplot.roc(rf_model_caret$pred$obs[indices],\n         rf_model_caret$pred$Abnormal[indices])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83ee5eeb70a71b616376504e06acb80eb3e3408f"},"cell_type":"code","source":"# Note once again the use of the stored model predictions\nroc(predictor=rf_model_caret$pred$Abnormal[indices],\n               response=rf_model_caret$pred$obs[indices],\n               levels=rev(levels(rf_model_caret$pred$obs[indices])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7958d0722e646d78b1bb863a8f4758355a4a074e"},"cell_type":"markdown","source":"Selecting those values where the `mtry = 8` has given us a value of 0.9136 for AUC. This still seems high, but much more plausible than 1.  Let's plot the curve using the same method as the first example, but this time filtered to include just one of the `mtry` permutations. \n\nThe main difference comes from me accessing the predictions made by the model during the cross-validation process (those that are kept through the `train_control`) and using the `predict()` function.  "},{"metadata":{"trusted":true,"_uuid":"9fbe0f672ccae70b70e653142734693781501e31"},"cell_type":"code","source":"colAUC(rf_model_caret$pred$Abnormal[indices], rf_model_caret$pred$obs[indices], plotROC = TRUE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18c892a435792bdd466f9c665827d487131566a5"},"cell_type":"markdown","source":"The `predict()` function will make a prediction using the final model, which has been retrained on all of the data; this might have had a huge impact on the performance of the model and lead to overfitting in a massive way. We can check this through comparing the accuracy of the predictions via `predict()` against the observed classes. "},{"metadata":{"trusted":true,"_uuid":"282fc099c6ee6b56a2d2c1f1c5c235cc16ca423e"},"cell_type":"code","source":"table(predict(rf_model_caret), caret_data$Class_att)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b54b3144f3a93596f39146118ac3c414c27ab5ad"},"cell_type":"markdown","source":"It wasn't that the AUC value of 1 was incorrect as such, but just that I hadn't spotted the difference in what I was comparing! I hope this has made sense and can save someone the effort that I've just gone through, but I am glad for the challenge and the opportunity to learn at the end of the day. \n\nThis prediction accuracy of 100% certainly indicates overfitting with the final model, and the one that we got during the cross-validation probably gives us a more true value for the out-of-sample predictive power of our model, but having that initial test/train split would have helped confirm that. "},{"metadata":{"_uuid":"374d65dc070a930bf615750549a16517e99dc8e9"},"cell_type":"markdown","source":"## Logistic Regression ROC curve \nLet's have a look at the ROC curve now for the logistic regression model that I made using `caret::train()`. If we use the same method that we used to create the incorrect ROC curve for the random forest then we get a high AUC score as we are using the final model.  \n\nWe didn't have any variable tuning parameters for the logistic regression so we don't have to filter the `pred` part of the model, we should just be able to do a direct comparison with the `Class_att` of the `caret_data`. "},{"metadata":{"trusted":true,"_uuid":"03273a67ebc6518f9a43b6eb7bbde21783408e0c"},"cell_type":"code","source":"colAUC(predictions$lr$Abnormal, caret_data$Class_att, plotROC = TRUE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b443b502b2f52446b249086417ac33d3e9f71f15"},"cell_type":"markdown","source":"Let's compare that to the model's internal class probs that were stored during the cross-validation process. "},{"metadata":{"trusted":true,"_uuid":"d61c539c70af8719b64a56c0ee6b938ee2f49d9b"},"cell_type":"code","source":"# Note use of model internal predictions\ncolAUC(log_model_caret$pred$Abnormal, log_model_caret$pred$obs, plotROC = TRUE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"873a2225765b20cd83c904514c629cffd2e52698"},"cell_type":"markdown","source":"We get a much more recognisable AUC value here, showing that our model performs better in the final version. This isn't surprising as the model is probably overfitted. It does highlight how good random forests are at overfitting however. "},{"metadata":{"_uuid":"15b6818e4b7449691861e52dc0af25b49d7e0cc8"},"cell_type":"markdown","source":"## SVM ROC curve\nI think I've covered the difference between the stored predictions made during the training process and the predictions made with the final model enough. Let's have a look at the ROC curve for our svm model that we built using `caret::train` and `svmLinear`."},{"metadata":{"trusted":true,"_uuid":"b45ed80f383afab2d2e811a0f6ad60392dae187e"},"cell_type":"code","source":"# Note use of model internal predictions\ncolAUC(svm_model_caret$pred$Abnormal, svm_model_caret$pred$obs, plotROC = TRUE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5db2e6c1a69a8e19895e5cbb3a91fffb7fb47aad"},"cell_type":"markdown","source":"- - - - \n# Preprocessing data with `caret` \nSome algorithms, such as KNN, depend on the magnitude of the values that are being used to predict in order to perform well; the data should be normalised and scaled to reach maximum capability from the algorithm. This isn't necessary true for algorithms like decision trees and random forests, but I'm keen to show how preprocessing can be done using `caret` in R. \n\nI will be focussing on 3 types of preprocessing that may improve the performance of our models. `center`, `scale` and `nzv`: centering, scaling and near zero variance. Centering and scaling the data are going to normalise the numeric variables, while near zero variance is going to work to simplify the model by disregarding those variables that have near zero variance, and thus don't improve the predictive power as they don't appear to differ between the classes. \n\nBelow I am going to do a set up and train our models, we can then look at them more closely afterwards.  \n\nBelow I show that we can preprocess the data before we train our model using the `preProcess` function and then applying it to the data using `predict`. Notice that the data has been centered and scaled. "},{"metadata":{"trusted":true,"_uuid":"a44586076c1f7a843f0e43c0aab3f63ad344b0a2"},"cell_type":"code","source":"head(caret_data)\n\nprocessing = preProcess(caret_data[-13], method = c(\"center\", \"scale\", \"nzv\"))\nprocessed_caret <- predict(processing, caret_data)\n\nhead(processed_caret)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c0c0a1918c632d1aa5fd1642ed4014223c96f46"},"cell_type":"markdown","source":"Now I will create a training control, and build some models using either the preprocessed data, or the original set that hasn't been touched before comparing the results. "},{"metadata":{"trusted":true,"_uuid":"1b0088e642229d576bd22ad31fd6b0ae0ce1c76c"},"cell_type":"code","source":"train_control <- caret::trainControl(method = 'cv', number = 5, savePredictions = 'final', classProbs = TRUE) \n\n# Logistic regression models\nset.seed(1)\nlr_processed <- caret::train(Class_att ~ ., data = processed_caret, trControl = train_control, method = 'glm', family = 'binomial')\n\nset.seed(1)\nlr_unprocessed <- caret::train(Class_att ~ ., data = caret_data, trControl = train_control, method = 'glm', family = 'binomial')\n\n# Random forest models\nset.seed(1)\nrf_processed <- caret::train(Class_att ~ ., data = processed_caret, trControl = train_control, method = 'rf', tuneLength = 10)\n\nset.seed(1)\nrf_unprocessed <- caret::train(Class_att ~ ., data = caret_data, trControl = train_control, method = 'rf', tuneLength = 10)\n\n#SVM models\nset.seed(1)\nsvm_processed <- caret::train(Class_att ~ ., data = processed_caret, trControl = train_control, method = 'svmLinear', tuneLength = 5)\n\nset.seed(1)\nsvm_unprocessed <- caret::train(Class_att ~ ., data = caret_data, trControl = train_control, method = 'svmLinear', tuneLength = 5)\n\n\nmods <- resamples(list(lr_processed = lr_processed, lr_unprocessed = lr_unprocessed, rf_processed = rf_processed, rf_unprocessed = rf_unprocessed, svm_processed = svm_processed, svm_unprocessed = svm_unprocessed))\nsummary(mods)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c4faed5368c263439722ef2b05dd5e17dabb077"},"cell_type":"markdown","source":"You can see that in these results that the models' performance didn't differ between the unprocessed and the processed data for training. The models that were created were completely identical. It makes me wonder whether this method is correct, but it could be down to the small data size that we are working with, or that the ranges in the data simply didn't differ enough to make a difference to the perceived accuracy. "},{"metadata":{"trusted":true,"_uuid":"d40eeb951cbc373e8c078eaea8453fbc97fa3002","_kg_hide-output":true},"cell_type":"code","source":"processing = preProcess(caret_data[-13], method = c(\"center\", \"scale\"))\nprocessing2 = preProcess(caret_data, method = c(\"center\", \"scale\"))\n\nprocessed_caret_1 <- predict(processing, caret_data)\nprocessed_caret_2 <- predict(processing2, caret_data)\n\nset.seed(1)\ntrain_control <- caret::trainControl(method = 'cv', number = 5, savePredictions = 'final', classProbs = TRUE) \n\nset.seed(1);knn_1  <- caret::train(Class_att ~ ., data = caret_data, trControl = train_control, method = 'knn')\nset.seed(1);knn_2 <- caret::train(Class_att ~ ., data = processed_caret_1, trControl = train_control, method = 'knn')\nset.seed(1);knn_3 <- caret::train(Class_att ~ ., data = processed_caret_2, trControl = train_control, method = 'knn')\n\n# To use preProcess then the first argument of train cannot be a recipe\nset.seed(1);knn_4 <- caret::train(x = caret_data[,1:12], y=caret_data$Class_att, trControl = train_control, method = 'knn', preProcess = c(\"center\", \"scale\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41ca14cb4f075ab122b263dc0f6b1ec924885291"},"cell_type":"markdown","source":"I've hidden the output above to save space in the kernel with the output, I can then jump straight to the important part below with the model comparison and that output. "},{"metadata":{"trusted":true,"_uuid":"f07a9450dbc51a8b8531d651884a1eb7717e7b8f"},"cell_type":"code","source":"summary(resamples(list(knn_1 = knn_1, knn_2 = knn_2, knn_3 = knn_3, knn_4 = knn_4)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34f0b8a5620b967efd024e35b0b119908c3f11a4"},"cell_type":"markdown","source":"I've created an example here with the different ways that I can think of for preprocessing with `caret`. \n\nIn my eyes, `knn_4`, `knn_2` and `knn_3` should be equivalent and produce the same outcome, yet they don't. `knn_2` and `knn_3` both use preprocessing prior to the training, while the method is passing in via the `preProcess` argument for `knn_4`. \n \nThis ultimately results in a different model because the preprocessing is applied on every cross validation fold. This results in different results as the observations are scaled over the 80% training set for each iterative fold, while the other two scale and center based on the entire data before the analysis has even begun. This is a small difference which took me some time to figure out but an important one in understanding the output.  \n\n**Note**:* If you increase the number of folds to be very high, this means that the training sets will be larger and nearer to the `nrow(data)` so the results will be more similar to those that preprocess the whole set. *"},{"metadata":{"_uuid":"55f9c3b3c5f19b4e2b77876fbb6989f9a71d61b5"},"cell_type":"markdown","source":"- - - -\n# Consistent cross-validation folds with `caret::createFolds`\nI have talked a lot about the process of building and training models with `caret` in this kernel, and have learnt a lot during the writing of it. Instead of going through and changing the kernel completely, I think it is more useful that I've documented the process of me realising things and my thoughts.  \nThis is why I haven't gone back and started again and replaced entire sections and text, but rather I've tried to show my journey of discovery. \n\nOne thing that I would have done, and will do in the future is create cross-validation folds prior to the model buidling process. Using a consistent `trainControl` is good and should still be used, but you can add a list of indices to `trainControl` that specify exactly which rows you should use for each cross-validation round. This acts as a strong factor in ensuring comparable model metrics as you can be sure of the training sets that are used when you call `caret::train`.   \n"},{"metadata":{"trusted":true,"_uuid":"6639451d69b38090d18b79ab7018ee92cb261248"},"cell_type":"code","source":"set.seed(1)\ncv_folds <- caret::createFolds(y = caret_data$Class_att, k = 5, returnTrain = TRUE)\nglimpse(cv_folds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82fecdca3bb122b15dc0e3f16458a59fb3a5966b"},"cell_type":"markdown","source":"You can see that we pass in the response variable as the argument `y`, as well as specifying the number of folds that we want to create (`k`). The `returnTrain` argument just means whether you want to return the test of the train indices, I have opted for the training sets here.  \n\nWe can then take these folds and pass them into the `trainControl` object as we are creating it ([full documentation](https://www.rdocumentation.org/packages/caret/versions/6.0-80/topics/trainControl)), ensuring that we are going to have the same training/test splits for each cross-validation fold when we train our models. \n\nFrom the documentation:  \n**index**: *a list with elements for each resampling iteration. Each list element is a vector of integers corresponding to the rows used for training at that iteration.*"},{"metadata":{"trusted":true,"_uuid":"44f7af381ce512818cbce6d1a857b6516b1edd1c"},"cell_type":"code","source":"our_train_control <- caret::trainControl(method = 'cv', \n                                         number = 5, \n                                         index = cv_folds, \n                                         savePredictions = 'final', \n                                         classProbs = TRUE) \n\nidentical(our_train_control$index, cv_folds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ed2a3640ab3b55eedf8854f63a544e80e9232ab"},"cell_type":"markdown","source":"After we have created our folds, and passed them to `trainControl` we can see that the same information about the cross-validation folds is contained within `our_train_control`. "},{"metadata":{"_uuid":"7d8c8b90a3b30c29f28fe51f59f145a5daa31fae"},"cell_type":"markdown","source":"- - - - \n<br>\n**More to come...**"},{"metadata":{"_uuid":"41cdf1f02d78bd62e59dabf083dd24a544ba85f5"},"cell_type":"markdown","source":"- - - - \n\nThanks for taking the time to have an explore of this data with me, and read through my initial thoughts and models. I plan on adding to this kernel in the coming weeks and months. If you have any suggestions, tips or advice about this, or potential additions, then I would be glad to hear them! \n\nPotential upcoming sections:\n- Suggestions welcome! \n\nIf you're interested in more `caret` content then you can check out another one of my kernels [here](https://www.kaggle.com/willcanniford/eda-and-predictions-using-caret-and-vtreat) where I explore craft beer data! \n\nIf you have enjoyed this, or learnt something, then please consider leaving an **upvote** to stay tuned for the upcoming content to this kernel! "},{"metadata":{"trusted":true,"_uuid":"d5c4d345612e2dc7975b6c01e6c666f87bdc93ea"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}